{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Apartment Price Prediction\n",
                "## 1. Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
                "from sklearn.metrics import mean_absolute_percentage_error\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.cluster import KMeans\n",
                "from sklearn.decomposition import TruncatedSVD\n",
                "from sklearn.ensemble import StackingRegressor\n",
                "from sklearn.linear_model import RidgeCV\n",
                "from sklearn.impute import SimpleImputer\n",
                "import xgboost as xgb\n",
                "import lightgbm as lgb\n",
                "import catboost as cb\n",
                "import re\n",
                "\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Loading & Optimized Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1020\n",
                        "Feature Engineering (Advanced)...Box\n",
                        "Processed shape: (5000, 98)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/var/folders/pt/4rypzgx161xbyqfj39t1nrqh0000gn/T/ipykernel_44033/2597888005.py:145: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
                        "  means = X_tr.groupby('loc_cluster').apply(lambda x: y_tr[x.index].mean())\n",
                        "/var/folders/pt/4rypzgx161xbyqfj39t1nrqh0000gn/T/ipykernel_44033/2597888005.py:145: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
                        "  means = X_tr.groupby('loc_cluster').apply(lambda x: y_tr[x.index].mean())\n",
                        "/var/folders/pt/4rypzgx161xbyqfj39t1nrqh0000gn/T/ipykernel_44033/2597888005.py:145: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
                        "  means = X_tr.groupby('loc_cluster').apply(lambda x: y_tr[x.index].mean())\n",
                        "/var/folders/pt/4rypzgx161xbyqfj39t1nrqh0000gn/T/ipykernel_44033/2597888005.py:145: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
                        "  means = X_tr.groupby('loc_cluster').apply(lambda x: y_tr[x.index].mean())\n",
                        "/var/folders/pt/4rypzgx161xbyqfj39t1nrqh0000gn/T/ipykernel_44033/2597888005.py:145: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
                        "  means = X_tr.groupby('loc_cluster').apply(lambda x: y_tr[x.index].mean())\n",
                        "/var/folders/pt/4rypzgx161xbyqfj39t1nrqh0000gn/T/ipykernel_44033/2597888005.py:155: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
                        "  full_means = X_train_raw.groupby('loc_cluster').apply(lambda x: y_log[x.index].mean())\n"
                    ]
                }
            ],
            "source": [
                "# Haversine Distance Function\n",
                "def haversine_distance(lat1, lon1, lat2, lon2):\n",
                "    R = 6371  # Earth radius in km\n",
                "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
                "    dphi = np.radians(lat2 - lat1)\n",
                "    dlambda = np.radians(lon2 - lon1)\n",
                "    a = np.sin(dphi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2)**2\n",
                "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
                "    return R * c\n",
                "\n",
                "# Layout Parsing\n",
                "def parse_layout(layout):\n",
                "    if pd.isna(layout): return 0, 0\n",
                "    rooms = re.search(r'(\\d+)', str(layout))\n",
                "    rooms = int(rooms.group(1)) if rooms else 1\n",
                "    kk = 1 if 'kk' in str(layout).lower() else 0\n",
                "    return rooms, kk\n",
                "\n",
                "# Condition Mapping\n",
                "def map_condition(cond):\n",
                "    mapping = {\n",
                "        'Nový': 5,\n",
                "        'Velmi dobrý': 4,\n",
                "        'Dobrý': 3,\n",
                "        'Udržovaný': 3,\n",
                "        'Po rekonstrukci': 4,\n",
                "        'Před rekonstrukcí': 2,\n",
                "        'Špatný': 1,\n",
                "        'Velmi špatný': 0,\n",
                "        'Ve výstavbě': 5,\n",
                "        'Projekt': 5\n",
                "    }\n",
                "    return mapping.get(cond, 3) # Default to average\n",
                "\n",
                "# Load Data\n",
                "train_df = pd.read_csv('appartments_train.csv')\n",
                "test_df = pd.read_csv('appartments_test.csv')\n",
                "print(len(test_df))\n",
                "\n",
                "# Separate target and log-transform it\n",
                "X = train_df.drop(columns=['price'])\n",
                "y = train_df['price']\n",
                "y_log = np.log1p(y)\n",
                "\n",
                "X_test = test_df.copy()\n",
                "if 'price' in X_test.columns:\n",
                "    X_test = X_test.drop(columns=['price'])\n",
                "\n",
                "# Combine\n",
                "combined = pd.concat([X, X_test], axis=0).reset_index(drop=True)\n",
                "\n",
                "print(\"Feature Engineering (Advanced)...Box\")\n",
                "\n",
                "# 1. Distance to Center\n",
                "combined['dist_center'] = haversine_distance(combined['gps_lat'], combined['gps_lon'], 50.0812, 14.4280)\n",
                "combined['dist_center_sq'] = combined['dist_center'] ** 2\n",
                "\n",
                "# 2. Layout Parsing\n",
                "combined[['n_rooms', 'has_kk']] = combined['layout'].apply(lambda x: pd.Series(parse_layout(x)))\n",
                "\n",
                "# 3. Condition Refinement\n",
                "combined['condition_num'] = combined['condition'].apply(map_condition)\n",
                "combined['is_very_bad'] = (combined['condition'] == 'Velmi špatný').astype(int)\n",
                "combined['is_new'] = (combined['condition'] == 'Nový').astype(int)\n",
                "\n",
                "# 4. Date Features\n",
                "combined['first_seen'] = pd.to_datetime(combined['first_seen'])\n",
                "combined['last_seen'] = pd.to_datetime(combined['last_seen'])\n",
                "min_date = combined['first_seen'].min()\n",
                "combined['days_since_first_seen'] = (combined['first_seen'] - min_date).dt.days\n",
                "combined['days_on_market'] = (combined['last_seen'] - combined['first_seen']).dt.days\n",
                "\n",
                "# 5. Text Mining (Optimized: 500 Features, 30 SVD)\n",
                "combined['text'] = combined['text'].fillna('').astype(str).str.lower()\n",
                "keywords = {\n",
                "    'luxus': r'luxus|nadstandard',\n",
                "    'rekonstrukce': r'rekonstrukc|zrekonstru',\n",
                "    'novostavba': r'novostavb|projekt',\n",
                "    'metro': r'metro',\n",
                "    'park': r'park',\n",
                "    'balkon': r'balkon|lodži|terasa',\n",
                "    'sklep': r'sklep|komora',\n",
                "    'garaz': r'garáž|parkování|stání',\n",
                "    'cihla': r'cihl'\n",
                "}\n",
                "for key, pattern in keywords.items():\n",
                "    combined[f'has_{key}'] = combined['text'].str.contains(pattern, regex=True).astype(int)\n",
                "\n",
                "tfidf = TfidfVectorizer(max_features=500, stop_words='english', ngram_range=(1, 2))\n",
                "text_features = tfidf.fit_transform(combined['text'])\n",
                "svd = TruncatedSVD(n_components=30, random_state=42)\n",
                "text_pca = svd.fit_transform(text_features)\n",
                "text_df = pd.DataFrame(text_pca, columns=[f'text_pca_{i}' for i in range(30)])\n",
                "combined = pd.concat([combined, text_df], axis=1)\n",
                "\n",
                "# 6. Geospatial Clustering (Optimized: 100 Clusters)\n",
                "coords = combined[['gps_lat', 'gps_lon']].fillna(combined[['gps_lat', 'gps_lon']].mean())\n",
                "kmeans = KMeans(n_clusters=100, random_state=42, n_init=10)\n",
                "combined['loc_cluster'] = kmeans.fit_predict(coords)\n",
                "\n",
                "# 7. Basic Cleaning\n",
                "fill_zero_cols = ['cellar_area', 'balcony_area', 'garden_area', 'parking']\n",
                "for col in fill_zero_cols:\n",
                "    combined[col] = combined[col].fillna(0)\n",
                "poi_nearest_cols = [c for c in combined.columns if 'nearest' in c]\n",
                "for col in poi_nearest_cols:\n",
                "    combined[col] = combined[col].fillna(combined[col].max() * 2.0)\n",
                "combined['elevator'] = combined['elevator'].fillna('Unknown')\n",
                "\n",
                "# 8. Ratios & Interactions\n",
                "combined['floor_ratio'] = combined['floor'] / combined['total_floors']\n",
                "combined['floor_ratio'] = combined['floor_ratio'].fillna(0)\n",
                "combined['total_area'] = combined['area'] + combined['cellar_area'] + combined['balcony_area'] + combined['garden_area']\n",
                "\n",
                "# Advanced Interactions\n",
                "combined['cond_area'] = combined['condition_num'] * combined['total_area']\n",
                "combined['cond_dist'] = combined['condition_num'] * combined['dist_center']\n",
                "combined['renov_potential'] = combined['is_very_bad'] * combined['dist_center'] # Location value for dumps\n",
                "combined['luxury_new'] = combined['is_new'] * combined['has_luxus']\n",
                "\n",
                "# Drop columns\n",
                "drop_cols = ['id', 'text', 'address', 'first_seen', 'last_seen']\n",
                "combined = combined.drop(columns=drop_cols)\n",
                "\n",
                "# Encode Categorical\n",
                "cat_cols = ['layout', 'construction', 'condition', 'ownership', 'elevator']\n",
                "combined = pd.get_dummies(combined, columns=cat_cols, drop_first=True)\n",
                "\n",
                "\n",
                "X_train_raw = combined.iloc[:len(X)].copy()\n",
                "X_test_raw = combined.iloc[len(X):].copy()\n",
                "\n",
                "kf_te = KFold(n_splits=5, shuffle=True, random_state=42)\n",
                "X_train_raw['cluster_target_enc'] = 0.0\n",
                "\n",
                "for train_idx, val_idx in kf_te.split(X_train_raw, y_log):\n",
                "    X_tr, X_val = X_train_raw.iloc[train_idx], X_train_raw.iloc[val_idx]\n",
                "    y_tr = y_log.iloc[train_idx]\n",
                "    \n",
                "    # Compute means\n",
                "    means = X_tr.groupby('loc_cluster').apply(lambda x: y_tr[x.index].mean())\n",
                "    \n",
                "    # Map to validation\n",
                "    X_train_raw.loc[X_train_raw.index[val_idx], 'cluster_target_enc'] = X_val['loc_cluster'].map(means)\n",
                "\n",
                "# Fill NaNs with global mean\n",
                "global_mean = y_log.mean()\n",
                "X_train_raw['cluster_target_enc'] = X_train_raw['cluster_target_enc'].fillna(global_mean)\n",
                "\n",
                "# For Test: Map using full training data\n",
                "full_means = X_train_raw.groupby('loc_cluster').apply(lambda x: y_log[x.index].mean())\n",
                "X_test_raw['cluster_target_enc'] = X_test_raw['loc_cluster'].map(full_means).fillna(global_mean)\n",
                "\n",
                "\n",
                "X_processed = X_train_raw.drop(columns=['loc_cluster'])\n",
                "X_test_processed = X_test_raw.drop(columns=['loc_cluster'])\n",
                "\n",
                "# Global Imputation\n",
                "imputer = SimpleImputer(strategy='median')\n",
                "X_processed_imputed = imputer.fit_transform(X_processed)\n",
                "X_test_processed_imputed = imputer.transform(X_test_processed)\n",
                "X_processed = pd.DataFrame(X_processed_imputed, columns=X_processed.columns)\n",
                "X_test_processed = pd.DataFrame(X_test_processed_imputed, columns=X_test_processed.columns)\n",
                "\n",
                "print(f\"Processed shape: {X_processed.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Stacking Ensemble with Optimized Hyperparameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Base Models (Optimized via Grid Search)\n",
                "xgb_model = xgb.XGBRegressor(\n",
                "    n_estimators=5000, \n",
                "    learning_rate=0.005, \n",
                "    max_depth=6, \n",
                "    subsample=0.8, \n",
                "    colsample_bytree=0.6, \n",
                "    reg_alpha=0.1, \n",
                "    reg_lambda=0.1,\n",
                "    random_state=42, \n",
                "    n_jobs=-1\n",
                ")\n",
                "\n",
                "lgb_model = lgb.LGBMRegressor(\n",
                "    n_estimators=3000, \n",
                "    learning_rate=0.005, \n",
                "    num_leaves=80, \n",
                "    subsample=0.8, \n",
                "    colsample_bytree=0.6, \n",
                "    reg_alpha=0.1, \n",
                "    reg_lambda=0.1,\n",
                "    random_state=42, \n",
                "    n_jobs=-1, \n",
                "    verbose=-1\n",
                ")\n",
                "\n",
                "cb_model = cb.CatBoostRegressor(\n",
                "    iterations=5000, \n",
                "    learning_rate=0.005, \n",
                "    depth=8, \n",
                "    l2_leaf_reg=3, \n",
                "    bagging_temperature=0.2,\n",
                "    random_state=42, \n",
                "    verbose=False, \n",
                "    allow_writing_files=False\n",
                ")\n",
                "\n",
                "# Meta Learner\n",
                "meta_learner = RidgeCV()\n",
                "\n",
                "estimators = [\n",
                "    ('xgb', xgb_model),\n",
                "    ('lgb', lgb_model),\n",
                "    ('cb', cb_model)\n",
                "]\n",
                "\n",
                "stacking_reg = StackingRegressor(\n",
                "    estimators=estimators,\n",
                "    final_estimator=meta_learner,\n",
                "    cv=5,\n",
                "    n_jobs=-1,\n",
                "    passthrough=True\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Final Prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training on 4917 samples (removed 83 outliers)...\n",
                        "Applied bias correction factor: 1.015\n",
                        "Submission saved to Data_nerds_predikce.csv\n"
                    ]
                }
            ],
            "source": [
                "# Refined Outlier Removal (Top 1% AND Bottom 0.5%)\n",
                "upper_limit = y_log.quantile(0.99)\n",
                "lower_limit = y_log.quantile(0.005)\n",
                "mask = (y_log < upper_limit) & (y_log > lower_limit)\n",
                "\n",
                "X_final_train = X_processed[mask]\n",
                "y_final_train = y_log[mask]\n",
                "\n",
                "print(f\"Training on {len(X_final_train)} samples (removed {len(X_processed) - len(X_final_train)} outliers)...\")\n",
                "\n",
                "stacking_reg.fit(X_final_train, y_final_train)\n",
                "final_preds_log = stacking_reg.predict(X_test_processed)\n",
                "final_preds = np.expm1(final_preds_log)\n",
                "\n",
                "# Bias Correction (Based on Rigorous Validation)\n",
                "# Validation showed systematic under-prediction (Bias ~ 0.985)\n",
                "correction_factor = 1.015\n",
                "final_preds = final_preds * correction_factor\n",
                "print(f\"Applied bias correction factor: {correction_factor}\")\n",
                "\n",
                "submission = pd.DataFrame({\n",
                "    'id': test_df['id'],\n",
                "    'price': final_preds\n",
                "})\n",
                "submission.to_csv('Data_nerds_predikce.csv', index=False)\n",
                "print(\"Submission saved to Data_nerds_predikce.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>id</th>\n",
                            "      <th>price</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>8795</td>\n",
                            "      <td>7.307805e+06</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>6516</td>\n",
                            "      <td>8.335489e+06</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>4714</td>\n",
                            "      <td>5.178632e+06</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>8423</td>\n",
                            "      <td>7.560859e+06</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>5361</td>\n",
                            "      <td>7.543292e+06</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "     id         price\n",
                            "0  8795  7.307805e+06\n",
                            "1  6516  8.335489e+06\n",
                            "2  4714  5.178632e+06\n",
                            "3  8423  7.560859e+06\n",
                            "4  5361  7.543292e+06"
                        ]
                    },
                    "execution_count": 54,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "submission.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
